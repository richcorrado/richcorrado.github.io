<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><!--This file was converted to xhtml by LibreOffice - see http://cgit.freedesktop.org/libreoffice/core/tree/filter/source/xslt for the code.--><head profile="http://dublincore.org/documents/dcmi-terms/"><meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8"/><title xml:lang="en-US">- no title specified</title><meta name="DCTERMS.title" content="" xml:lang="en-US"/><meta name="DCTERMS.language" content="en-US" scheme="DCTERMS.RFC4646"/><meta name="DCTERMS.source" content="http://xml.openoffice.org/odf2xhtml"/><meta name="DCTERMS.issued" content="2017-03-06T08:32:16.373052438" scheme="DCTERMS.W3CDTF"/><meta name="DCTERMS.modified" content="2017-03-06T11:51:55.519895789" scheme="DCTERMS.W3CDTF"/><meta name="DCTERMS.provenance" content="" xml:lang="en-US"/><meta name="DCTERMS.subject" content="," xml:lang="en-US"/><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/" hreflang="en"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/" hreflang="en"/><link rel="schema.DCTYPE" href="http://purl.org/dc/dcmitype/" hreflang="en"/><link rel="schema.DCAM" href="http://purl.org/dc/dcam/" hreflang="en"/><style type="text/css">
	@page {  }
	table { border-collapse:collapse; border-spacing:0; empty-cells:show }
	td, th { vertical-align:top; font-size:12pt;}
	h1, h2, h3, h4, h5, h6 { clear:both }
	ol, ul { margin:0; padding:0;}
	li { list-style: none; margin:0; padding:0;}
	<!-- "li span.odfLiEnd" - IE 7 issue-->
	li span. { clear: both; line-height:0; width:0; height:0; margin:0; padding:0; }
	span.footnodeNumber { padding-right:1em; }
	span.annotation_style_by_filter { font-size:95%; font-family:Arial; background-color:#fff000;  margin:0; border:0; padding:0;  }
	* { margin:0;}
	.P1 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P10 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P11 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P12 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P13 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P14 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P15 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P16 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P17 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P2 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P21 { font-size:25.5pt; font-weight:bold; margin-bottom:0.0835in; margin-top:0.1665in; font-family:Thorndale; writing-mode:page; }
	.P22 { font-size:13.5pt; font-weight:bold; margin-bottom:0.0835in; margin-top:0.0835in; font-family:Liberation Serif; writing-mode:page; }
	.P23 { font-size:13.5pt; font-weight:bold; margin-bottom:0.0835in; margin-top:0.0835in; font-family:Liberation Serif; writing-mode:page; padding-left:0in; padding-right:0in; padding-top:0in; padding-bottom:0.0291in; border-left-style:none; border-right-style:none; border-top-style:none; border-bottom-width:0.0133cm; border-bottom-style:solid; border-bottom-color:#000000; }
	.P3 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P4 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P5 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P6 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P7 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P8 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.P9 { font-size:12pt; line-height:120%; margin-bottom:0.0972in; margin-top:0in; font-family:Liberation Serif; writing-mode:page; }
	.Emphasis { font-style:italic; }
	.Internet_20_link { color:#000080; text-decoration:underline; }
	.T10 { font-size:18pt; font-weight:bold; }
	.T11 { font-size:18pt; font-weight:bold; }
	.T12 { font-size:18pt; font-weight:bold; }
	.T6 { font-size:18pt; font-weight:bold; }
	.T7 { font-size:18pt; font-weight:bold; }
	.T8 { font-size:18pt; font-weight:bold; }
	.T9 { font-size:18pt; font-weight:bold; }
	<!-- ODF styles with no properties representable as CSS -->
	.T1 .T13 .T14 .T15 .T16 .T17 .T18 .T19 .T2 .T20 .T21 .T22 .T23 .T24 .T25 .T3 .T4 .T5  { }
	</style></head><body dir="ltr" style="max-width:8.5in;margin-top:0.3937in; margin-bottom:0.3937in; margin-left:0.7874in; margin-right:0.3937in; background-color:transparent; "><p class="P21"><span class="T25">Ka</span>ggle House Prices <span class="T1">challenge</span></p><h4 class="P22"><a id="a__Richard_Corrado"><span/></a><span class="Emphasis">Richard Corrado</span></h4><h4 class="P23"><a id="a__March_6__2017"><span/></a><span class="Emphasis"><span class="T1">March 6</span></span><span class="Emphasis">, 2017</span></h4><p class="P1"><span class="Emphasis"><span class="T1">This is an overview of the approach used on the Kaggle House Prices challenge. The training dataset consists of 1460 house sale price observations with 81 features, while the test set consists of 1459 houses with sale price omitted.  The metric is RMSE of the log of the sale price.</span></span></p><p class="P5"><span class="Emphasis"><span class="T6">Data Cleaning</span></span></p><p class="P2"><span class="Emphasis"><span class="T2">In the R Markdown file </span></span><a href="http://EDA_plus_feateng.html/" class="Internet_20_link"><span class="Emphasis"><span class="T2">EDA_plus_feateng.</span></span></a><span class="Emphasis"><span class="T2">Rmd,  I do an exhaustive exploratory data analysis.  I find that around 30 of the feature columns contain missing entries, so the first steps are to determine the nature of the missing data.  Where appropriate, I use decision trees with the rpart package to impute missing values using a subset of relevant features to  predict the missing values.  I expect that this is slightly better than using mean/median or nearest neighbors. </span></span></p><p class="P4"><span class="Emphasis"><span class="T4">A complication here is that 11 features correspond to the houses’ basement, while around 80 houses from the combined training + test set have no basement at all.  Similarly,  7 features correspond to the garage, while around 159 houses have no garage.   In the analysis, I explore fitting models to the various subsets I can obtain from NoBasement and NoGarage boolean features.</span></span></p><p class="P3"><span class="Emphasis"><span class="T3">I also examine every feature for typos or mutual inconsistencies.  For instance, there is a feature called MSSubClass that is a number corresponding to an industry code for the type of dwelling.  In many cases, this feature was found to be inconsistent with another feature called HouseStyle, which contained roughly the same information stored as a factor whose levels were the dwelling types.  In reconciling errors, I assumed that the HouseStyle error was less prone to coding error, since the levels were words rather than numerical codes.</span></span></p><p class="P5"><span class="Emphasis"><span class="T6">Feature Engineering</span></span></p><p class="P5"><span class="Emphasis"><span class="T5">In a second round through the list of features, I consider the tidy data obtained in the first round and begin engineering new features.</span></span></p><p class="P5"><span class="Emphasis"><span class="T5">For categorical features, I generally used two types of encoding. First, uniformly I used one-hot encoding via the caret package in a round of preprocessing in a later stage of the analysis.  However, during the second round, in most cases, I computed median log(SalePrice) values for each level and  also defined an ordinal encoding based on the median prices.  The corresponding ordinals for the SaleCondition, GarageQual and MSZoning turned out to be top 20 features in my Lasso models. The Neighborhood feature had 25 levels, so I also defined another feature assigning houses to each of 6 bins based on median price.</span></span></p><p class="P6"><span class="Emphasis"><span class="T13">After dealing with all of the categorical features, I move on to analyzing the original numerical features.  I start by analyzing the correlation between the response and the features.  </span></span></p><p class="P6"><span class="Emphasis"><span class="T13">GrLivArea, corresponding to the above grade living area of the house, had a 0.7 correlation with log(SalePrice).   Based on the correlation and the general idea that house size is an important component of house price, it was expected to be a strong regressor.   I found the frequency distribution of the variable to be </span></span><span class="Emphasis"><span class="T13">skewed, as well as pressure from outliers at high area and low price.  After taking the log(GrLivArea), I found that much of the pressure from outliers was removed.  However, I still identified 4 houses in the training set with extremely large area that were lying far away from the log-linear trend, as well as 5 houses with lower areas that had extremely low prices compared to the trend.   This set of 9 points were considered outliers and I later examined the effect of their removal on the validation error for linear models.  Ultimately I selected a dataset where 5 outliers had been removed to use as a basis for my kaggle submissions.</span></span></p><p class="P8"><span class="Emphasis"><span class="T15">For the other numerical variables, I included log versions whenever that improved skewness.  I also found that, in several cases, I could improve linearity by engineering a new feature that was a linear combination of the feature and its square root.</span></span></p><p class="P7"><span class="Emphasis"><span class="T14">The presence of houses without garage or basement caused problems for several of the associated features.  In particular, a numerical feature like GarageArea has an accumulation around 0.  In this particular case, these values did not seem to put an overly large amount of pressure on a linear fit.</span></span></p><p class="P8"><span class="Emphasis"><span class="T15">In addition to examining the subsets based on presence of a garage or basement, I also engineered various features based on the sums of the numerical features.  These had the benefit of not suffering from accumulation around zero, even when basement or garage features were included.   I also included log versions and linearized versions where appropriate.</span></span></p><p class="P8"><span class="Emphasis"><span class="T7">Final Preprocessing</span></span></p><p class="P8"><span class="Emphasis"><span class="T15">I then performed a final round of preprocessing.  To begin, I used caret to convert my categorical variables to one-hot encoding.  This step balloons the number of features to over 400.</span></span></p><p class="P8"><span class="Emphasis"><span class="T15">During my EDA, I noticed some category feature levels were  present in the training set, but not in the test set.  Since I don’t want my models to learn features that can’t be used to predict over the training set, I had to remove the corresponding one-hot features from the data. </span></span></p><p class="P9"><span class="Emphasis"><span class="T16">Next I looked for near zero-variance predictors and features with extremely low statistics.  I chose to drop any feature that had 5 or less observations in the training set, since I could not have statistical confidence in parameters learned from such a small sample.</span></span></p><p class="P9"><span class="Emphasis"><span class="T16">I then removed features which had &gt; 0.99 correlation with other features.  This was mainly due to one-hot variables that had perfect correlation with NoGarage and NoBasement.</span></span></p><p class="P9"><span class="Emphasis"><span class="T16">I were left with 346 features, of which roughly half are one-hot features of varying sparseness.</span></span></p><p class="P10"><span class="Emphasis"><span class="T16">Later I added a section to the Rmd file to make it easy to generate datasets in which some subset of outliers had been removed.  In particular, this allowed us to refit engineered features to the retained data.  These tidy datasets were saved in csv format for later use.</span></span></p><p class="P10"><span class="Emphasis"><span class="T8">Analysis of Outliers</span></span></p><p class="P10"><span class="Emphasis"><span class="T16"> Further analysis was done in python using scikit-learn and associated toolkits.  First was an analysis of the outliers I had identified in the EDA of GrLivArea.  This was done in the notebook </span></span><a href="http://DetectOutliers.html/" class="Internet_20_link"><span class="Emphasis"><span class="T16">DetectOutliers.ipynb</span></span></a><span class="Emphasis"><span class="T16">. For validation purposes, I made a validation set using 20% of the training data.  I then chose the LassoLarsCV model and RMSE as my metric. </span></span></p><p class="P11"><span class="Emphasis"><span class="T17">The best validation error came from dropping 5 outliers, which resulted in a 6% improvement in RMSE.    There were a large number of datasets that had a similar performance.  </span></span></p><p class="P11"><span class="Emphasis"><span class="T9">Feature Selection</span></span></p><p class="P11"><span class="Emphasis"><span class="T17">During my feature engineering, I created many features that were highly correlated with other original or new features.  For the most part, I were planning on using regularized models to avoid problems arising from this.  However, I felt that I should still consider whether models would be improved by dropping some features by hand.  This analysis was performed for the best dataset in </span></span><a href="http://FeatureSelection-111001001.html/" class="Internet_20_link"><span class="Emphasis"><span class="T17">FeatureSelection-111001001.ipynb</span></span></a><span class="Emphasis"><span class="T17">. </span></span></p><p class="P12"><span class="Emphasis"><span class="T18">The first thing I note in the this notebook is that the baseline validation error after dropping the five outliers and then refitting the engineered features is now almost 13%.  I attribute this improvement entirely to the refit on the engineered features, many of which depended heavily on the GrLivArea feature.</span></span></p><p class="P12"><span class="Emphasis"><span class="T18">Next, I find highly correlated sets of features and examine the effect on validation error by refitting in the absence of subsets of them.   In the end, I identify a set of 4 features which, if dropped, results in a 1.6% increase in validation error.    </span></span></p><p class="P12"><span class="Emphasis"><span class="T10">Modeling </span></span></p><p class="P13"><span class="Emphasis"><span class="T19">Next I selected various candidate models:</span></span></p><p class="P13"><span class="Emphasis"><span class="T19">Linear models:  Ridge, Lasso, Lasso Lars, Elastic Net, Orthogonal Matching Pursuit</span></span></p><p class="P13"><span class="Emphasis"><span class="T19">Tree models: Random Forest, XGBoost</span></span></p><p class="P13"><span class="Emphasis"><span class="T19">Support Vector Regressor</span></span></p><p class="P13"><span class="Emphasis"><span class="T19">Multilevel Perceptron Regressor</span></span></p><p class="P13"><span class="Emphasis"><span class="T21">This was done in the notebook </span></span><a href="http://Modeling-111001001.html/" class="Internet_20_link"><span class="Emphasis"><span class="T21">Modeling-111001001.ipynb</span></span></a><span class="Emphasis"><span class="T21">. My strategy was to again choose a 20% validation set.  Then I used random search and grid search in 10-fold cross-validation to select optimal hyperparameters over the 80% training set.  The resulting models, together with optimal hyperparameters, were used to fit the training set and their validation error computed.</span></span></p><p class="P13"><span class="Emphasis"><span class="T19">My general results were that linear models had the best performance, likely due to all of the linearized features that I engineered.  The nonlinear models XGBoost and MLP were slightly behind LassoLARS,  I also decided against using datasets where I dropped features.  </span></span></p><p class="P14"><span class="Emphasis"><span class="T11">Subsetting</span></span></p><p class="P14"><span class="Emphasis"><span class="T20">With promising models, I returned to the question of how to best deal with houses with no basement or no garage. This was analyzed in </span></span><a href="file:///home/rcorrado/code/kaggle/housepricesART/Subsetting+111001001.html" class="Internet_20_link"><span class="Emphasis"><span class="T20">Subsetting 111001001.ipynb</span></span></a><span class="Emphasis"><span class="T20"> using the same validation strategy as before.  </span></span></p><p class="P15"><span class="Emphasis"><span class="T22">In my total training set (with outliers removed), there were 80 houses with no basement.  Fits on 80% of this subset had a worse validation error on the remaining 20% than I obtained from the baseline of no subsetting. Similarly subsetting to the 37 houses with no garage obtained worse results than the baseline.  This could be expected from the low statistics involved.</span></span></p><p class="P15"><span class="Emphasis"><span class="T22">I found 1345 houses with both a garage and basement.  Validation error on this subset was 7% better than the baseline.  Therefore I chose a strategy where I subsetted the houses with both a garage and basement for a best case fit.  Then I generated predictions for the remaining subset of houses with no basement or garage by fitting a model on the total training data, which was my baseline.</span></span></p><p class="P15"><span class="Emphasis"><span class="T22">I generated submissions for 3 models:</span></span></p><p class="P15"><span class="Emphasis"><span class="T22">1. Lasso Lars alone.   This had a Private Leaderboard score of 0.12700 which placed in only the top 30% of submissions.</span></span></p><p class="P16"><span class="Emphasis"><span class="T23">2. Blended Lasso Lars, XGBoost and MLP. Blending was done by an average using the inverse validation error as weights. This model had a Private Leaderboard score of </span></span><span class="Emphasis">0.12242 </span><span class="Emphasis"><span class="T23">which would have been a top 16% submission.  However, the Public Leaderboard score was worse than the other submissions, so I did not select it.</span></span></p><p class="P16"><span class="Emphasis"><span class="T23">3. Blend of Lasso Lars and Elastic Net.  This has a Private Leaderboard score of </span></span><span class="Emphasis">0.12740, </span><span class="Emphasis"><span class="T23">which was worse than Lasso Lars alone.</span></span></p><p class="P16"><span class="Emphasis"><span class="T12">Conclusions</span></span></p><p class="P13"><span class="Emphasis"><span class="T23">This was an excellent project to learn many data science techniques. The data set was messy enough to require extensive tidying, but small enough that this was manageable.  However there is no doubt that the small size of the dataset precluded some of the subsetting techniques that  I might have used to lower our validation errors.</span></span></p><p class="P17"><span class="Emphasis"><span class="T24">The nature of the problem and the features allowed a large amount of feature engineering to be done.  Early modeling with less features had linear models performing behind the tree and other nonlinear models.  Given more time, I would like to explore how tree models like XGBoost behave when we remove subsets of features.  </span></span></p><p class="P13"><span class="Emphasis"/></p><p class="P13"><span class="Emphasis"/></p><p class="P13"><span class="Emphasis"/></p><p class="P8"><span class="Emphasis"/></p><p class="P3"><span class="Emphasis"/></p><p class="P1"><span class="Emphasis"/></p></body></html>